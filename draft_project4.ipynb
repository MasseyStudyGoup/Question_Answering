{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pylab import rcParams\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# things we need for NLP\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "# things we need for Tensorflow\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn_cell\n",
    "from tensorflow.python.ops import rnn\n",
    "import chardet\n",
    "import numpy as np\n",
    "import struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our intents file\n",
    "import json\n",
    "\n",
    "def getdata(path):\n",
    "    g = open(path,'r')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "\n",
    "intents = list(getdata('/data/QA/QA_Video_Games.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english') + [\n",
    "    '.',\n",
    "    ',',\n",
    "    '--',\n",
    "    '\\'s',\n",
    "    '?',\n",
    "    ')',\n",
    "    '(',\n",
    "    ':',\n",
    "    '\\'',\n",
    "    '\\'re',\n",
    "    '\"',\n",
    "    '-',\n",
    "    '}',\n",
    "    '{',\n",
    "    u'—',\n",
    "    '',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import unicodedata\n",
    "def CleanLines(text):\n",
    "    cleanLine = []\n",
    "    identify = string.maketrans('', '')\n",
    "    delEStr = string.punctuation +string.digits\n",
    "    for i in text:         \n",
    "        i = i.encode(\"utf-8\")\n",
    "        lines = i.translate(identify,delEStr)\n",
    "        cleanLine.append(lines) \n",
    "    return cleanLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uni(text):\n",
    "    uni_text = []\n",
    "    for i in text:\n",
    "        i = unicode(i, \"utf-8\")\n",
    "        uni_text.append(i)\n",
    "    return uni_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deuni(text):\n",
    "    deuni_text = []\n",
    "    for i in text:\n",
    "        i = i.encode(\"utf-8\")\n",
    "        deuni_text.append(i)\n",
    "    return deuni_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete long words\n",
    "def long_word_filter(words):\n",
    "    word_list = []\n",
    "    for i in words:\n",
    "        if len(i)<15:\n",
    "            word_list.append(i)\n",
    "        \n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7744, 'documents')\n",
      "(1183, 'classes')\n",
      "(13772, 'unique stemmed words')\n",
      "(28893, 'answers')\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "answers = [] ##是question-answer pair\n",
    "\n",
    "# loop through each question in our intents\n",
    "for intent in intents:\n",
    "    for question in intent['questions']:\n",
    "        # tokenize each word in the sentence\n",
    "        #text = CleanLines(question['questionText'])\n",
    "        w = nltk.word_tokenize(question['questionText'])\n",
    "        w = CleanLines(w)\n",
    "        w = uni(w)\n",
    "        # add to our words list\n",
    "        words.extend(w)\n",
    "        documents.append((w, intent['asin']))\n",
    "        # add to our classes list\n",
    "        if intent['asin'] not in classes:\n",
    "            classes.append(intent['asin'])\n",
    "        \n",
    "        for answer in question['answers']:\n",
    "            w2 = nltk.word_tokenize(answer['answerText'])\n",
    "            w2 = CleanLines(w2)\n",
    "            w2 = uni(w2)\n",
    "            words.extend(w2)\n",
    "            answers.append((w,w2))\n",
    "\n",
    "# stem and lower each word and remove duplicates\n",
    "words = [stemmer.stem(w.lower()) for w in words if w not in stop_words]\n",
    "words = sorted(list(set(words)))\n",
    "words = deuni(words)\n",
    "words = long_word_filter(words)\n",
    "\n",
    "# remove duplicates\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "print (len(documents), \"documents\")\n",
    "print (len(classes), \"classes\")\n",
    "print (len(words), \"unique stemmed words\")\n",
    "print (len(answers), \"answers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(words, sg=1, size=100,  window=5,  min_count=5,  negative=3, sample=0.001, hs=1, workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('/data/QA/Game.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_seqs = []\n",
    "answer_seqs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有的参数没用\n",
    "max_w = 50\n",
    "float_size = 4\n",
    "word_vector_dict = {}\n",
    "word_vec_dim = 100\n",
    "max_seq_len = 8\n",
    "word_set = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = open('/data/QA/Game.bin', \"rb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answers是question-answer pair\n",
    "questions = []\n",
    "for i in range(len(answers)):\n",
    "    question = deuni(answers[i][0])\n",
    "    questions.append(question) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#answer1是 单纯answer\n",
    "answers1 =[]\n",
    "for i in range(len(answers)):\n",
    "    answer = deuni(answers[i][1])\n",
    "    answers1.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo = open(\"/data/QA/QA_pair.txt\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(questions)):\n",
    "    for w_q in questions[i]:\n",
    "        w_q = w_q.lower()\n",
    "        fo.write( w_q+' ')\n",
    "        \n",
    "    fo.write('|')\n",
    "    \n",
    "    for w_a in answers1[i]:\n",
    "        w_a = w_a.lower()\n",
    "        fo.write(w_a+' ')\n",
    "    \n",
    "    fo.write('\\n')\n",
    "            \n",
    "fo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28893\n"
     ]
    }
   ],
   "source": [
    "fo = open(\"/data/QA/QA_pair.txt\", \"r\")\n",
    "print  len(fo.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load vector\n",
    "model_w2v = Word2Vec.load('/data/QA/Game.bin')\n",
    "word_vector=model_w2v.wv\n",
    "#word_vocab = model_w2v.vocabulary\n",
    "word_vector_dict = word_vector.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_seq(input_file):\n",
    "    \"\"\"读取切好词的文本文件，加载全部词序列\n",
    "    \"\"\"\n",
    "    file_object = open(input_file, 'r')\n",
    "    vocab_dict = {}\n",
    "    while True:\n",
    "        question_seq = []\n",
    "        answer_seq = []\n",
    "        line = file_object.readline()\n",
    "        if line:\n",
    "            line_pair = line.split('|')\n",
    "            line_question = line_pair[0]\n",
    "            line_answer = line_pair[1]\n",
    "            for word in line_question.split(' '):\n",
    "                if word_vector_dict.has_key(word):\n",
    "                    question_seq.append(word_vector[word])\n",
    "            for word in line_answer.decode('utf-8').split(' '):\n",
    "                if word_vector_dict.has_key(word):\n",
    "                    answer_seq.append(word_vector[word])\n",
    "        else:\n",
    "            break\n",
    "        question_seqs.append(question_seq)\n",
    "        answer_seqs.append(answer_seq)\n",
    "    file_object.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn_cell\n",
    "from tensorflow.python.ops import rnn\n",
    "import chardet\n",
    "import numpy as np\n",
    "import struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:18: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "# get the vectorized question & answer \n",
    "question_seqs=[]\n",
    "answer_seqs=[]\n",
    "init_seq('/data/QA/QA_pair.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_seq_len(seqs):\n",
    "    seq_lens = []\n",
    "    for seq in seqs:\n",
    "        seq_lens.append(len(seq))\n",
    "        seq_lens.sort(reverse=True)\n",
    "    return seq_lens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 164\n"
     ]
    }
   ],
   "source": [
    "max_q_seq = get_max_seq_len(question_seqs)\n",
    "max_a_seq = get_max_seq_len(answer_seqs)\n",
    "print max_q_seq,max_a_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "211"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_len = max_q_seq + max_a_seq\n",
    "max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySeq2Seq(object):\n",
    "    def __init__(self, max_seq_len = 164, word_vec_dim = 100, input_file='/data/QA/QA_pair.txt'):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.word_vec_dim = word_vec_dim\n",
    "        self.input_file = input_file\n",
    "    \n",
    "    def generate_trainig_data(self):\n",
    " #       load_word_set()\n",
    " #       load_vectors(\"/data/QA/Game_words.bin\")\n",
    "        init_seq(self.input_file)\n",
    "        xy_data = []\n",
    "        y_data = []\n",
    "        for i in range(len(question_seqs)):\n",
    "        #for i in range(100):\n",
    "            question_seq = question_seqs[i]\n",
    "            answer_seq = answer_seqs[i]\n",
    "            if len(question_seq) < self.max_seq_len and len(answer_seq) < self.max_seq_len:\n",
    "                #多余的位设为0，与question的reverse合并，为什么要reverse？ * - repeat ；+ - 合并\n",
    "                sequence_ry = [np.zeros(self.word_vec_dim)] * (self.max_seq_len-len(question_seq)) + list(reversed(question_seq))\n",
    "                #多余的位设为0， 与answer合并\n",
    "                sequence_y = answer_seq + [np.zeros(self.word_vec_dim)] * (self.max_seq_len-len(answer_seq))\n",
    "                #合并\n",
    "                sequence_xy = sequence_ry + sequence_y\n",
    "                sequence_y = [np.ones(self.word_vec_dim)] + sequence_y\n",
    "                xy_data.append(sequence_xy)\n",
    "                y_data.append(sequence_y)\n",
    "\n",
    "                #print \"right answer\"\n",
    "                #for w in answer_seq:\n",
    "                #    (match_word, max_cos) = vector2word(w)\n",
    "                #    if len(match_word)>0:\n",
    "                #        print match_word, vector_sqrtlen(w)\n",
    "\n",
    "        return np.array(xy_data), np.array(y_data)\n",
    "    \n",
    "    \n",
    "    def model(self, feed_previous=False):\n",
    "        # 通过输入的XY生成encoder_inputs和带GO头的decoder_inputs\n",
    "        input_data = tflearn.input_data(shape=[None, self.max_seq_len*2, self.word_vec_dim], dtype=tf.float32, name = \"XY\")\n",
    "        encoder_inputs = tf.slice(input_data, [0, 0, 0], [-1, self.max_seq_len, self.word_vec_dim], name=\"enc_in\")\n",
    "        decoder_inputs_tmp = tf.slice(input_data, [0, self.max_seq_len, 0], [-1, self.max_seq_len-1, self.word_vec_dim], name=\"dec_in_tmp\")\n",
    "        go_inputs = tf.ones_like(decoder_inputs_tmp)\n",
    "        go_inputs = tf.slice(go_inputs, [0, 0, 0], [-1, 1, self.word_vec_dim])\n",
    "        decoder_inputs = tf.concat(1, [go_inputs, decoder_inputs_tmp], name=\"dec_in\")\n",
    "\n",
    "        # 编码器\n",
    "        # 把encoder_inputs交给编码器，返回一个输出(预测序列的第一个值)和一个状态(传给解码器)\n",
    "        (encoder_output_tensor, states) = tflearn.lstm(encoder_inputs, self.word_vec_dim, return_state=True, scope='encoder_lstm')\n",
    "        encoder_output_sequence = tf.pack([encoder_output_tensor], axis=1)\n",
    "\n",
    "        # 解码器\n",
    "        # 预测过程用前一个时间序的输出作为下一个时间序的输入\n",
    "        # 先用编码器的最后一个输出作为第一个输入\n",
    "        if feed_previous:\n",
    "            first_dec_input = go_inputs\n",
    "        else:\n",
    "            first_dec_input = tf.slice(decoder_inputs, [0, 0, 0], [-1, 1, self.word_vec_dim])\n",
    "        decoder_output_tensor = tflearn.lstm(first_dec_input, self.word_vec_dim, initial_state=states, return_seq=False, reuse=False, scope='decoder_lstm')\n",
    "        decoder_output_sequence_single = tf.pack([decoder_output_tensor], axis=1)\n",
    "        decoder_output_sequence_list = [decoder_output_tensor]\n",
    "        # 再用解码器的输出作为下一个时序的输入\n",
    "        for i in range(self.max_seq_len-1):\n",
    "            if feed_previous:\n",
    "                next_dec_input = decoder_output_sequence_single\n",
    "            else:\n",
    "                next_dec_input = tf.slice(decoder_inputs, [0, i+1, 0], [-1, 1, self.word_vec_dim])\n",
    "            decoder_output_tensor = tflearn.lstm(next_dec_input, self.word_vec_dim, return_seq=False, reuse=True, scope='decoder_lstm')\n",
    "            decoder_output_sequence_single = tf.pack([decoder_output_tensor], axis=1)\n",
    "            decoder_output_sequence_list.append(decoder_output_tensor)\n",
    "\n",
    "        decoder_output_sequence = tf.pack(decoder_output_sequence_list, axis=1)\n",
    "        real_output_sequence = tf.concat(1, [encoder_output_sequence, decoder_output_sequence])\n",
    "\n",
    "        net = tflearn.regression(real_output_sequence, optimizer='sgd', learning_rate=0.1, loss='mean_square')\n",
    "        model = tflearn.DNN(net)\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        trainXY, trainY = self.generate_trainig_data()\n",
    "        model = self.model(feed_previous=False)\n",
    "        model.fit(trainXY, trainY, n_epoch=1000, snapshot_epoch=False, batch_size=1)\n",
    "        model.save('/data/QA/model_tensorflow')\n",
    "        return model\n",
    "    \n",
    "    def load(self):\n",
    "        model = self.model(feed_previous=True)\n",
    "        model.load('/data/QA/model_tensorflow')\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = MySeq2Seq( max_seq_len = 164, word_vec_dim = 100, input_file='/data/QA/QA_pair.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:18: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-63d4e052a972>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-269dee9b6f69>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mtrainXY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_trainig_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_previous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainXY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msnapshot_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-269dee9b6f69>\u001b[0m in \u001b[0;36mgenerate_trainig_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;31m#        print match_word, vector_sqrtlen(w)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
